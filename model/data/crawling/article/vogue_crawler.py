import requests
from bs4 import BeautifulSoup
import json
from concurrent.futures import ThreadPoolExecutor, as_completed

def crawl_vogue_article(url):
    """
    ë³´ê·¸ í•œêµ­ ì‚¬ì´íŠ¸ì—ì„œ ì œëª©, ë³¸ë¬¸, ë‚ ì§œë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    
    Args:
        url (str): í¬ë¡¤ë§í•  ê¸°ì‚¬ URL
        
    Returns:
        dict: ì œëª©, ë³¸ë¬¸, ë‚ ì§œë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
    """
    try:
        # ìš”ì²­ í—¤ë” ì„¤ì •
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # ì›¹í˜ì´ì§€ ìš”ì²­
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        
        if response.status_code != 200:
            return {"error": f"HTTP Error: {response.status_code}"}
        
        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title = None
        title_tag = soup.find('h1')
        if title_tag:
            title = title_tag.get_text(strip=True)
        
        # ë‚ ì§œ ì¶”ì¶œ
        date = None
        # ë©”íƒ€ ë°ì´í„°ì—ì„œ ë‚ ì§œ ì°¾ê¸°
        date_meta = soup.find('meta', property='article:published_time')
        if date_meta:
            date = date_meta.get('content', '').split('T')[0]
        
        # ë©”íƒ€ ë°ì´í„° ì‹¤íŒ¨ì‹œ ë‹¤ë¥¸ ë°©ì‹ ì‹œë„
        if not date:
            script_tags = soup.find_all('script', type='application/ld+json')
            for script in script_tags:
                try:
                    import json
                    data = json.loads(script.string)
                    if 'datePublished' in data:
                        date = data['datePublished'].split('T')[0]
                        break
                except:
                    pass
        
        # ë§ˆì§€ë§‰ ì‹œë„: í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ ì°¾ê¸°
        if not date:
            date_patterns = soup.find_all(string=lambda text: text and len(text.strip()) > 0)
            for text in date_patterns:
                text_clean = text.strip()
                if text_clean and (text_clean.startswith('2026.') or text_clean.startswith('2025.')):
                    date = text_clean
                    break
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        content = ""
        article_body = soup.find('article')
        if not article_body:
            article_body = soup.find('div', class_=lambda x: x and 'article' in x.lower())
        if not article_body:
            article_body = soup.find('div', class_=lambda x: x and 'content' in x.lower())
        
        if article_body:
            paragraphs = article_body.find_all('p')
            content = '\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
        
        # ê²°ê³¼ ë°˜í™˜
        result = {
            "title": title,
            "date": date,
            "content": content,
            "url": url
        }
        
        return result
        
    except requests.exceptions.RequestException as e:
        return {"error": f"Request error: {str(e)}"}
    except Exception as e:
        return {"error": f"Parsing error: {str(e)}"}


def save_to_json(data, filename="vogue_article.json"):
    """
    í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        data (dict): ì €ì¥í•  ë°ì´í„°
        filename (str): ì €ì¥í•  íŒŒì¼ëª…
    """
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"âœ“ ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def crawl_multiple_urls(urls, max_workers=5):
    """
    ì—¬ëŸ¬ URLì„ ë™ì‹œì— í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    
    Args:
        urls (list): í¬ë¡¤ë§í•  URL ë¦¬ìŠ¤íŠ¸
        max_workers (int): ë™ì‹œì— ì‹¤í–‰í•  ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜
        
    Returns:
        list: í¬ë¡¤ë§ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  URLì— ëŒ€í•´ í¬ë¡¤ë§ ì‘ì—… ì œì¶œ
        future_to_url = {executor.submit(crawl_vogue_article, url): url for url in urls}
        
        # ì™„ë£Œëœ ì‘ì—…ë¶€í„° ì²˜ë¦¬
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                result = future.result()
                results.append(result)
                print(f"âœ“ ì™„ë£Œ: {url}")
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ({url}): {str(e)}")
                results.append({"error": f"Request error: {str(e)}", "url": url})
    
    return results


if __name__ == "__main__":
    # í¬ë¡¤ë§í•  URL ë¦¬ìŠ¤íŠ¸ (ì—¬ê¸°ì— ì›í•˜ëŠ” URLì„ ì¶”ê°€í•˜ì„¸ìš”)
    urls = [
        "https://www.vogue.co.kr/?p=746528",
        "https://www.vogue.co.kr/?p=742818",
        "https://www.vogue.co.kr/?p=735902",
        "https://www.vogue.co.kr/?p=712849",
        "https://www.vogue.co.kr/?p=713047",
        "https://www.vogue.co.kr/?p=706860",
        "https://www.vogue.co.kr/?p=699207",
        "https://www.vogue.co.kr/?p=691391",
        "https://www.vogue.co.kr/?p=679131", #0122 18:42 

    ]
    
    print(f"ğŸš€ {len(urls)}ê°œ URL ë™ì‹œ í¬ë¡¤ë§ ì‹œì‘...")
    print("-" * 50)
    
    # ì—¬ëŸ¬ URL ë™ì‹œ í¬ë¡¤ë§
    results = crawl_multiple_urls(urls, max_workers=5)
    
    print("-" * 50)
    print(f"âœ“ ì´ {len(results)}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ")
    
    # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    save_to_json(results, "vogue_articles.json")
    
    # í„°ë¯¸ë„ì— ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\n" + "="*50)
    print("í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½:")
    for i, article in enumerate(results, 1):
        if "error" not in article:
            print(f"\n[{i}] {article['title']}")
            print(f"    ë‚ ì§œ: {article['date']}")
            print(f"    URL: {article['url']}")
        else:
            print(f"\n[{i}] âŒ ì˜¤ë¥˜: {article['url']}")
            print(f"    {article['error']}")
